{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "Once again, to review, descriptive statistics refers to the statistical tools used to summarize a dataset. One of the first operations often used to get a sense of what a given data looks like is the `mean` operation. \n",
    "\n",
    "\n",
    "## Mean \n",
    "\n",
    "You know what the mean is, you've heard it every time your computer science professor handed your midterms back and announced that the average, or *mean* was a disappointing low of 59. Woops. \n",
    "\n",
    "With that said, the “average” is just one of many summary statistics you might choose to describe the typical value or the **central tendency** of a sample. You can find the formal mathematical definition right below:\n",
    "\n",
    "$$ \\mu = \\frac{1}{n} \\Sigma_i x_i $$\n",
    "\n",
    "Computing the mean isn't a fun task, especially if you have hundreds, even *thousands* of data points to compute the mean for. You definitely don't want to do this by hand, right? \n",
    "\n",
    "Right. In Python, you can either implement your own mean function, or you can use `numpy`. We'll begin with our own implementation so you can get a thorough understanding of how these sorts of functions are implemented. Below, `t` is a list of datapoints, so you can use built-in list functions such as `sum()` and `len()` to compute the total sum and divide by the total number of data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "In the same way that the mean is intended to describe the central tendency, variance is intended to describe the <b>spread</b>. \n",
    "\n",
    "##### varaince formula goes here\n",
    "\n",
    "The x<sub>i</sub> - &mu; is called the \"deviation from the mean\", making the variance the squared deviation multiplied by 1 over the number of samples. This is why the square root of the variance, &sigma;, is called the <b>standard deviation</b>.\n",
    "\n",
    "Using the mean function we created above, we'll write up a function that calculates the variance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Sampling Distribution?\n",
    "\n",
    "A sampling distribution is the probability distribution of a large number of samples drawn from a specific type of population. It provides us with the distribution of frequencies of different outcomes. From this, we can infer the probability of seeing a given data point, given our parameters (&theta;). This is written as p(X|&theta;). For example, we might have data on 1,000 coin flips, where 1 indicates a head and 0 indicates a tail.\n",
    "\n",
    "In R, this might look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "flips = np.random.randint(2, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives us 1,000 samples of 0s and 1s. Now, let's output the mean of this data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense -- the average of 0s and 1s should be roughly .5 since they should occur at roughly the same frequency!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution\n",
    "\n",
    "As we said in the previous section, a sampling distribution specifies how the data is generated. For our coin flips, specifically, we can think of our data as being generated from a Bernoulli Distribution. This is because Bernoulli Distributions include samples that are discrete and binary, such as 0s and 1s, yes or nos.\n",
    "\n",
    "With that said, we can create samples from this distribution with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bernoulli_flips = np.random.binomial(n=1, p=.5, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the the type of distribution is referred to as \"binomial\" here. `n` refers to the number of labels from 0 to n there will be. In this example, `n=1`, so we have labels of 0s and 1s. `p` is the probability parameter, which in this case is `.5` because we want an equivalent probability for each coin. Now let's see what the mean gets us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just as expected! \n",
    "\n",
    "\n",
    "Now that we have defined how we believe our data were generated, we can calculate the probability of seeing our data given our parameters. Since we have selected a Bernoulli distribution, we only have one parameter, p. \n",
    "\n",
    "We can use the PMF of the Bernoulli distribution to get our desired probability for a single coin flip. Recall that the PMF takes a single observed data point and then given the parameters (p in our case) returns the probablility of seeing that data point given those parameters. \n",
    "\n",
    "For a Bernoulli distribution it is simple: if the data point is a 1, the PMF returns p. If the data point is a 0, it returns (1-p). We can easily do this with `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Recall the Bernoulli Distribution from above. Write a function `bern_pmf` that takes a data point and the probability parameter as input and outputs the probability of that data occuring in a Bernoulli Distribution. Make sure your function handles datapoints that aren't 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "\n",
    "The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples. The alternative is a continuous distribution, which is characterized by a CDF that is a continuous function (as opposed to a step function).\n",
    "\n",
    "The normal distribution, also called Gaussian, is the most commonly used because it describes so many scenarios. Despite its wide range of applicability, CDFs with the normal distribution are non-trivial compared to other distributions.\n",
    "\n",
    "Unlike the other distributions we will look at, there is no closed-form expression for the normal CDF. Instead, we write it in terms of the error function, erf(x). \n",
    "\n",
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/df8ef831fe542320ad73386cd662a67c14999bff \"Logo Title Text 1\")\n",
    "\n",
    "Now, using the scipy module, we can create the CDF for a Normal Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "\n",
    "def NormalCdf(x):\n",
    "    return (erf(x / root2) + 1) / 2\n",
    "\n",
    "def NormalCdf(x, mu=0, sigma=1):\n",
    "    return(StandardNormalCdf(float(x - mu) / sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we imported `er` from `scipy.special`. This CDF, when plotted, looks like:\n",
    "\n",
    "##### cdf plot goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Write a function `exp_cdf()` that takes x and a series of events as input and returns the probability of that x occuring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "Up until now we have used the symbol &mu; for both the sample mean and the mean parameter for the sake of simplicity. Now we will distinguish them, using x&#772; for the sample mean. The process of acquiring this &mu; is called <b>estimation</b>. On the other hand, we have <b>estimators</b>, which is just a statistic used to estimate a parameter.\n",
    "\n",
    "More often than not, we use the sample mean to estimate &mu; but what if we introduce outliers?\n",
    "\n",
    "One option is to identify and discard the outliers and then compute the sample mean of the rest. Otherwise, you can simply use the median as an estimator.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Using the sample mean to estimate &mu; is fairly intuitive, but suppose we introduce outliers. One option is to identify and discard outliers, then compute the sample mean of the rest. Another option is to use the median as an estimator.\n",
    "\n",
    "\n",
    "# Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] 11.20 11.75 11.27  8.84  9.23 10.35  8.42  9.52 10.67  9.31\n"
     ]
    }
   ],
   "source": [
    "sample_vec<- c(11.20,  11.75,  11.27,   8.84,   9.23, 10.35,   8.42,   9.52,  10.67,   9.31)\n",
    "print(sample_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  A random sample, sample_vec, was randomly generated with a mean, &mu;,  of 10 and standard deviation, &sigma;, 1.  We can use this sample vector to find the estimation had we not been given the mean.  Calculate the sample mean - call this `mean1`.  Calcualate the sample median - call this `median1`.  Notice how close these values are to 10.\n",
    "\n",
    "2.  Replace the first value of the vector with the value 100 - call this `vector2`.  Calculate the sample mean - call this `mean2`.  Calcualate the sample median - call this `median2`.  Notice how close these values are to 10 and the effect of an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "A statistical hypothesis is a statistical test used to determine whether there is enough evidence in a sample of data to conclude that a certain condition is true for the <b>entire</b> population. The underlying logic is similar to a proof by contradiction. To prove a mathematical statement, A, you assume temporarily that A is false. If that assumption leads to a contradiction, you conclude that A must actually be true.\n",
    "\n",
    "Similarly, to test a hypothesis like, “This effect is real,” we assume, temporarily, that is is not. That’s the <b>null hypothesis</b>, which is what you typically want to disprove. Based on that assumption, we compute the probability of the apparent effect. That’s the <b>p-value</b>. If the p-value is low enough, we conclude that the null hypothesis is unlikely to\n",
    "be true.\n",
    "\n",
    "## Z-Values, P-Values & Tables\n",
    "\n",
    "These three values are associated with standard normal distributions. Z-values are a measure of how many standard deviations away from mean is the observed value. P-values are the probabilities, which you can retrieve from its associated z-value in a [z-table](http://www.stat.ufl.edu/~athienit/Tables/Ztable.pdf). \n",
    "\n",
    "We've already reviewed how to retrieve the p-value, but how do we get the z-value? With the following formula:\n",
    "\n",
    "##### z val formula goes here\n",
    "\n",
    "where x is your data point, &mu; is the mean and &sigma; is the standard deviation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing Steps\n",
    "\n",
    "The initial step to hypothesis testing is to actually set up the Hypothesis, both the NULL and Alternate. \n",
    "\n",
    "Next, you set the criteria for decision. To set the criteria for a decision, we state the level of significance for a test. Based on the level of significance, we make a decision to accept the Null or Alternate hypothesis.\n",
    "\n",
    "The third step is to compute the random chance of probability. Higher probability has higher likelihood and enough evidence to accept the Null hypothesis.\n",
    "\n",
    "Lastly, you make a decision. Here, we compare p value with predefined significance level and if it is less than significance level, we reject Null hypothesis, else we accept it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Blood glucose levels for obese patients have a mean of 100 with a standard deviation of 15. A researcher thinks that a diet high in raw cornstarch will have a positive effect on blood glucose levels. A sample of 36 patients who have tried the raw cornstarch diet have a mean glucose level of 108. Test the hypothesis that the raw cornstarch had an effect or not.\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "First, we have to state the hypotheses. We set our NULL Hypothesis to be the glucose variable = 100 since it is initially assumed to be true. The alternative is that the glucose variable is greater than 100.\n",
    "\n",
    "\n",
    "### Significance Level\n",
    "\n",
    "Unless specified, we typically set the significance level to 5%, or `0.05`. Now, if we figure out the corresponding z-value from the [z-table](http://www.stat.ufl.edu/~athienit/Tables/Ztable.pdf), we'll see that it corresponds to `1.645`. This is now the z-score cut off for significance level, meaning the area to the right (or z-scores higher than 1.645) is the rejection hypothesis space. \n",
    "\n",
    "### Computation\n",
    "\n",
    "Now, we can compute the random chance probability using z scores and the z-table. Recall the formula from earlier, z = (x - &mu;)/ &sigma;. Now, before we go into computing, let's overview the difference between standard deviation of the mean and standard deviation of the distribution. \n",
    "\n",
    "When we want to gain a sense the precision of the mean, we calculate what is called the <i>sample distribution of the mean</i>. Assuming statistical independence, the standard deviation of the mean is related to the standard deviation of the distribution with the formula &sigma; <sub>mean</mean> = &sigma; / &radic;N. \n",
    "\n",
    "With that knowledge in mind, we've been given the standard deviation of the distribution, but we need the standard deviation of the mean instead. So before we begin calculating the z value, we plug in the values for the formula above. Then we get &sigma; <sub>mean</sub> = 15 / &radic;36, or `2.5`.\n",
    "\n",
    "Now we have all the needed information to compute the z value:\n",
    "\n",
    "```\n",
    "z = (108-100) / 2.5 = 3.2\n",
    "```\n",
    "\n",
    "\n",
    "### Hypotheses \n",
    "\n",
    "Awesome! Now we compare this value to the z-value from before, 1.645. 3.2 is clearly greater than 1.645, which means it's in the rejection hypothesis space. That tells us that we can we reject the Null hypothesis. Therefore, there <i>is</i> an effect from the raw starch.\n",
    "\n",
    "\n",
    "# Challenge\n",
    "\n",
    "1.  You have a coin and you would like to check whether it is fair or not.  Let `theta` be the probability of heads.  We have two hypothesis.  The null hypothesis is that the could is fair.  What value would we define `theta` to be? -  call this `nulltheta`.  The alternative hypthesis is then the coin is not fair.  (Thus, the alternative hypothetis is not equal to `nulltheta`.)\n",
    "\n",
    "2.  Again let our significance level be 5% or 0.05.  In this case, we are looking at two -tails.  Using the z-table from above, calculate the positive z-value.- call this `zval`.  This means we will accept the null hypothesis for z-scores between -`zval` to +`zval`.  We will accept the alternative hypthosis for z-scores from -infinity to -`zval` or +`zval` to infinity.  \n",
    "\n",
    "3.  To test the null hypthosis and alternative hypothesis, we toss a coin 100 times and record the number of heads.  Calculate the mean. - call this `mean`.  Calculate the the standard deviation.- call this `standev`. (hint: this is a binomial distribution.)\n",
    "\n",
    "4.  Let's look at `coin1`. Let's say we toss one coin, `coin1`, 100 times and we get 45 heads. Calculate this z - score. - call it `zs1`. Does this accept the null hypotheis or the alternative hypothesis.  Label  `coin1` as 'null' or 'alternative'.  \n",
    "\n",
    "5.  Let's look at `coin2`. Let's say we toss another coin, `coin2`, 100 times and we get 61 heads. Calculate this z - value. - call it `zs2`.  Does this accept the null hypotheis or the alternative hypothesis.  Label  `coin2` as 'null' or 'alternative'.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "The formal meaning of a confidence interval is that 95% of the confidence intervals should, in the long run, contain the true population parameter. Mathematically, this is defined as $z$-$\\sigma$  or $z$-standard-deviation **confidence interval** (where $z \\ge 0$) to be the interval\n",
    "\n",
    "$$ [\\overline X - zs, \\overline X + zs] \\,.$$\n",
    "\n",
    "Assuming the mean estimate is normally distributed, then we can use the statistics of the normal distribution to compute the probability that the mean estimate falls within the $z$-$\\sigma$ confidence interval.  If $n(x\\mid\\mu,\\sigma)$ is the normal pdf with mean $\\mu$ and standard deviation $\\sigma$, then the probability the mean falls within the $z$-$\\sigma$ confidence interval is\n",
    "\n",
    "$$ \\int_{\\overline X - zs}^{\\overline X + zs} n(x \\mid \\overline X, s)dx = \\int_{- z}^{z} n(x \\mid 0, 1)dx = N(z) - N(-z) $$\n",
    "\n",
    "where $N$ is the cumulative normal distribution.  We usually choose $z$ to be 2, indicating the ~95% confidence interval, or 3, indicating the ~99% confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
